{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Results Analysis\n",
    "*Note: If you are prompted to select a kernel, please select SageMaker Jumpstart PyTorch 1.0*\n",
    "\n",
    "In this notebook we will analyze the results from the model we trained.\n",
    "\n",
    "You can select Run->Run All Cells from the menu to run all cells in Studio (or Cell->Run All in a SageMaker Notebook Instance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "\n",
    "from sagemaker.analytics import HyperparameterTuningJobAnalytics\n",
    "from sagemaker.pytorch import PyTorchModel\n",
    "from source.visualization.model_visualisation_utils import get_dfs_from_hpt, plot_df_list, get_best_training_job\n",
    "\n",
    "with open(\"stack_outputs.json\") as f:\n",
    "    sagemaker_configs = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get data from Hyperparameter Tuning job\n",
    "Enter your HPO job name to visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_hpo_job(name_contains):\n",
    "    sagemaker_client = boto3.client('sagemaker')\n",
    "    response = sagemaker_client.list_hyper_parameter_tuning_jobs(\n",
    "        SortBy='CreationTime',\n",
    "        NameContains=name_contains,\n",
    "        StatusEquals='Completed'\n",
    "    )\n",
    "    training_jobs = response['HyperParameterTuningJobSummaries']\n",
    "    assert len(training_jobs) > 0, \\\n",
    "        'Hyperparameter tuning job has not yet completed. To view its progress open https://console.aws.amazon.com/sagemaker/ and select \\\n",
    "         Hyperparameter tuning jobs. Job name should contain {}.'.format(name_contains)\n",
    "    latest_training_job = training_jobs[0]    \n",
    "    return latest_training_job['HyperParameterTuningJobName']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hpo_job_name = get_latest_hpo_job(name_contains=\"sagemaker-soln-fpm\")\n",
    "hpt = HyperparameterTuningJobAnalytics(hpo_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = hpt.training_job_summaries()\n",
    "dfs = get_dfs_from_hpt(summaries, metrics=[\"Epoch\", \"train_auc\", \"train_acc\", \"train_loss\",\n",
    "                                           \"test_auc\", \"test_acc\", \"test_loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize metrics of all the jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df_list(dfs, metric_name=\"test_acc\", y_label=\"Test accuracy\", min_final_value=0.70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize data from the best Training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_job_name, best_job_df = get_best_training_job(dfs, \"test_auc\", \"maximize\")\n",
    "best_job_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 3, figsize=(1500/108, 400/108), dpi=108)\n",
    "\n",
    "axs[0].plot(best_job_df[\"Epoch\"], best_job_df[\"test_loss\"], label=best_job_name)\n",
    "axs[0].set_xlabel(\"Epochs\")\n",
    "axs[0].set_ylabel(\"Test loss\")\n",
    "axs[0].grid(color=\"0.9\", linestyle='-', linewidth=3)\n",
    "\n",
    "axs[1].plot(best_job_df[\"Epoch\"], best_job_df[\"test_auc\"], label=best_job_name)\n",
    "axs[1].set_xlabel(\"Epochs\")\n",
    "axs[1].set_ylabel(\"Test AUC\")\n",
    "axs[1].grid(color=\"0.9\", linestyle='-', linewidth=3)\n",
    "\n",
    "axs[2].plot(best_job_df[\"Epoch\"], best_job_df[\"test_acc\"], label=best_job_name)\n",
    "axs[2].set_xlabel(\"Epochs\")\n",
    "axs[2].set_ylabel(\"Test Accuracy\")\n",
    "axs[2].grid(color=\"0.9\", linestyle='-', linewidth=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build endpoint for the best training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = sagemaker_configs[\"S3Bucket\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sage_session = sagemaker.session.Session()\n",
    "model_artifact = sage_session.describe_training_job(best_job_name)[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "model_artifact = os.path.join(os.path.dirname(model_artifact), \"output.tar.gz\")\n",
    "print(\"Building endpoint with model {}\".format(model_artifact))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "role = sagemaker_configs['SageMakerIamRole']\n",
    "\n",
    "model = PyTorchModel(model_data=model_artifact,\n",
    "                     role=role,\n",
    "                     entry_point=\"inference.py\",\n",
    "                     source_dir=\"source/dl_utils\",\n",
    "                     framework_version='1.5.0',\n",
    "                     py_version = 'py3',\n",
    "                     name=sagemaker_configs[\"SageMakerModelName\"],\n",
    "                     code_location=\"s3://{}/endpoint\".format(s3_bucket)\n",
    "                    )\n",
    "\n",
    "endpoint_instance_type = sagemaker_configs[\"SageMakerInferenceInstanceType\"]\n",
    "\n",
    "predictor = model.deploy(initial_instance_count=1, instance_type=endpoint_instance_type, endpoint_name=sagemaker_configs[\"SageMakerEndpointName\"])\n",
    "\n",
    "def custom_np_serializer(data):\n",
    "    return json.dumps(data.tolist())\n",
    "    \n",
    "def custom_np_deserializer(np_bytes, content_type='application/x-npy'):\n",
    "    out = np.array(json.loads(np_bytes.read()))\n",
    "    return out\n",
    "\n",
    "predictor.serializer = custom_np_serializer\n",
    "predictor.deserializer = custom_np_deserializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "\n",
    "config = botocore.config.Config(read_timeout=200)\n",
    "runtime = boto3.client('runtime.sagemaker', config=config)\n",
    "\n",
    "data = np.ones(shape=(1, 20, 2)).tolist()\n",
    "payload = json.dumps(data)\n",
    "\n",
    "response = runtime.invoke_endpoint(EndpointName=sagemaker_configs[\"SageMakerEndpointName\"],\n",
    "                                   ContentType='application/json',\n",
    "                                   Body=payload)\n",
    "out = json.loads(response['Body'].read().decode())[0]\n",
    "\n",
    "print(\"Given the sample input data, the predicted probability of failure is {:0.2f}%\".format(100*(1.0-out[0]))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Our solution is easily customizable. You can customize the:\n",
    "\n",
    "* Input data visualizations.\n",
    "  * See [`sagemaker/3_data_visualization.ipynb`](sagemaker/3_data_visualization.ipynb).\n",
    "* Machine learning.\n",
    "  * See [`sagemaker/source/train.py`](sagemaker/source/train.py).\n",
    "  * See [`sagemaker/source/dl_utils/network.py`](sagemaker/source/dl_utils/network.py).\n",
    "* Dataset processing.\n",
    "  * See [`sagemaker/1_introduction.ipynb`](sagemaker/1_introduction.ipynb) on how to define the config file.\n",
    "\n",
    "Additionally, you can change configuration in the config file located [here](./config/config.yaml).\n",
    "The default configuration is as follows:\n",
    "\n",
    "* `fleet_info_fn=data/example_fleet_info.csv`\n",
    "* `fleet_sensor_logs_fn=data/example_fleet_sensor_logs.csv`\n",
    "* `vehicle_id_column=vehicle_id`\n",
    "* `timestamp_column=timestamp`\n",
    "* `target_column=target`\n",
    "* `period_ms=30000`\n",
    "* `dataset_size=10000`\n",
    "* `window_length=20`\n",
    "* `chunksize=10000`\n",
    "* `processing_chunksize=1000`\n",
    "* `fleet_dataset_fn=data/processed/fleet_dataset.csv`\n",
    "* `train_dataset_fn=data/processed/train_dataset.csv`\n",
    "* `test_dataset_fn=data/processed/test_dataset.csv`\n",
    "* `period_column=period_ms`\n",
    "\n",
    "`fleet_info_fn`, `fleet_sensor_logs_fn`, `fleet_dataset_fn`, `train_dataset_fn`, `test_dataset_fn` defines the location of dataset files.\n",
    "\n",
    "`vehicle_id_column`, `timestamp_column`, `target_column`, and `period_column` defines the headers for columns\n",
    "\n",
    "`dataset_size`, `chunksize`, `processing_chunksize`, `period_ms`, and `window_length` defines properties of the dataset.\n",
    "\n",
    "## Feature Requests and Contributions\n",
    "\n",
    "See our [GitHub repository](https://github.com/awslabs/aws-fleet-predictive-maintenance) to make feature requests and contributions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (PyTorch JumpStart)",
   "language": "python",
   "name": "HUB_1P_IMAGE"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}